{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from tqdm.auto import tqdm\n",
    "import torchaudio\n",
    "from dataclasses import dataclass, field\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../data'\n",
    "TRAIN_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-train.csv')\n",
    "TRAIN_DATA_BASE = f\"{ROOT_DIR}/cv-valid-train\"\n",
    "\n",
    "TEST_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-test.csv')\n",
    "TEST_DATA_BASE = f\"{ROOT_DIR}/cv-valid-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-train/sample-000000.mp3</td>\n",
       "      <td>learn to recognize omens and follow them the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-train/sample-000001.mp3</td>\n",
       "      <td>everything in the universe evolved he said</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-train/sample-000002.mp3</td>\n",
       "      <td>you came so that you could learn about your dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-train/sample-000003.mp3</td>\n",
       "      <td>so now i fear nothing because it was those ome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-train/sample-000004.mp3</td>\n",
       "      <td>if you start your emails with greetings let me...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195771</th>\n",
       "      <td>cv-valid-train/sample-195771.mp3</td>\n",
       "      <td>the englishman said nothing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>male</td>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195772</th>\n",
       "      <td>cv-valid-train/sample-195772.mp3</td>\n",
       "      <td>the irish man sipped his tea</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195773</th>\n",
       "      <td>cv-valid-train/sample-195773.mp3</td>\n",
       "      <td>what do you know about that</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195774</th>\n",
       "      <td>cv-valid-train/sample-195774.mp3</td>\n",
       "      <td>the phone rang while she was awake</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195775</th>\n",
       "      <td>cv-valid-train/sample-195775.mp3</td>\n",
       "      <td>among these people were a couple of cyclists a...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195776 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "0       cv-valid-train/sample-000000.mp3   \n",
       "1       cv-valid-train/sample-000001.mp3   \n",
       "2       cv-valid-train/sample-000002.mp3   \n",
       "3       cv-valid-train/sample-000003.mp3   \n",
       "4       cv-valid-train/sample-000004.mp3   \n",
       "...                                  ...   \n",
       "195771  cv-valid-train/sample-195771.mp3   \n",
       "195772  cv-valid-train/sample-195772.mp3   \n",
       "195773  cv-valid-train/sample-195773.mp3   \n",
       "195774  cv-valid-train/sample-195774.mp3   \n",
       "195775  cv-valid-train/sample-195775.mp3   \n",
       "\n",
       "                                                     text  up_votes  \\\n",
       "0       learn to recognize omens and follow them the o...         1   \n",
       "1              everything in the universe evolved he said         1   \n",
       "2       you came so that you could learn about your dr...         1   \n",
       "3       so now i fear nothing because it was those ome...         1   \n",
       "4       if you start your emails with greetings let me...         3   \n",
       "...                                                   ...       ...   \n",
       "195771                        the englishman said nothing         1   \n",
       "195772                       the irish man sipped his tea         1   \n",
       "195773                        what do you know about that         1   \n",
       "195774                 the phone rang while she was awake         2   \n",
       "195775  among these people were a couple of cyclists a...         4   \n",
       "\n",
       "        down_votes       age gender   accent  duration  \n",
       "0                0       NaN    NaN      NaN       NaN  \n",
       "1                0       NaN    NaN      NaN       NaN  \n",
       "2                0       NaN    NaN      NaN       NaN  \n",
       "3                0       NaN    NaN      NaN       NaN  \n",
       "4                2       NaN    NaN      NaN       NaN  \n",
       "...            ...       ...    ...      ...       ...  \n",
       "195771           0  thirties   male  england       NaN  \n",
       "195772           0       NaN    NaN      NaN       NaN  \n",
       "195773           0       NaN    NaN      NaN       NaN  \n",
       "195774           0  twenties   male       us       NaN  \n",
       "195775           1       NaN    NaN      NaN       NaN  \n",
       "\n",
       "[195776 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_INDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the audio from corresponding path\n",
    "2. resample the audio to 16kHz\n",
    "3. Ensure the audio is mono channel, if not, take the mean of the channels\n",
    "4. using wav2vec processor to process the audio\n",
    "5. remove all punctuation from the transcription (match the original training data style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, audio_base_path):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.audio_base_path = audio_base_path\n",
    "        self.chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"“%”\\\\\\]'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        audio_filename = row['filename']\n",
    "        text = row['text']\n",
    "\n",
    "        # Construct full audio file path\n",
    "        audio_path = os.path.join(self.audio_base_path, audio_filename)\n",
    "\n",
    "        # Load audio file\n",
    "        try:\n",
    "            speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "            return None # Return None for problematic samples, will be filtered by collator\n",
    "\n",
    "        # Resample if necessary (Wav2Vec2 expects 16kHz)\n",
    "        if sampling_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            speech_array = resampler(speech_array)\n",
    "        \n",
    "        # Ensure mono audio\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = speech_array.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Process audio (normalize, etc.)\n",
    "        # Squeeze to remove the channel dimension (e.g., from (1, samples) to (samples,))\n",
    "        input_values = self.processor(speech_array.squeeze(0).numpy(), sampling_rate=16000).input_values[0]\n",
    "\n",
    "        # Clean and tokenize text\n",
    "        text = re.sub(self.chars_to_ignore_regex, '', text).lower()\n",
    "    \n",
    "        return {\"input_values\": input_values, \"labels\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Filter out None values from __getitem__\n",
    "        features = [f for f in features if f is not None]\n",
    "        if not features: # If all features were None, return empty batch\n",
    "            return None\n",
    "\n",
    "        # Separate input_values and labels\n",
    "        input_features = [feature[\"input_values\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"][0] for feature in features]\n",
    "   \n",
    "        batch = self.processor(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "  \n",
    "        labels_batch = self.processor(\n",
    "            text = label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME, sampling_rate=16000)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME)\n",
    "    RANDOM_STATE = 42 # to ensure reproducibility\n",
    "    TRAIN_SPLIT_RATIO = 0.7\n",
    "    LEARNING_RATE = 1e-4\n",
    "    BATCH_SIZE = 1 # Keep batch size as 1 for simplicity in this example\n",
    "    NUM_EPOCHS = 1\n",
    "    LOG_STEP = 200\n",
    "\n",
    "    # Lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    log_steps_recorded = [] # To keep track of which steps the losses correspond to\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        TRAIN_INDICES, test_size=1-TRAIN_SPLIT_RATIO, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "\n",
    "    # Freeze feature extractor layers\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_dataset = AudioDataset(train_indices, processor, TRAIN_DATA_BASE)\n",
    "    val_dataset = AudioDataset(val_indices, processor, TRAIN_DATA_BASE)\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        total_train_loss_epoch = 0 # This will still track the total loss for the epoch average\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\")):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_values=batch[\"input_values\"], labels=batch[\"labels\"])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_train_loss_epoch += loss.item() # Accumulate for epoch average\n",
    "\n",
    "            # --- Logging and Validation Phase (Every LOG_STEP) ---\n",
    "            if (i + 1) % LOG_STEP == 0:\n",
    "                # Log the loss of the current batch/step directly\n",
    "                current_batch_train_loss = loss.item()\n",
    "                train_losses.append(current_batch_train_loss)\n",
    "                \n",
    "                model.eval() # Set model to evaluation mode\n",
    "                total_val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in tqdm(val_dataloader, desc=f\"Step {i+1} Validation\"):\n",
    "                        val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
    "                        outputs = model(input_values=val_batch[\"input_values\"], labels=val_batch[\"labels\"])\n",
    "                        val_loss = outputs.loss\n",
    "                        total_val_loss += val_loss.item()\n",
    "                avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                log_steps_recorded.append(i + 1)\n",
    "                \n",
    "                print(f\"Step {i+1} - Train Loss (current batch): {current_batch_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "                model.train() # Set model back to training mode\n",
    "\n",
    "        avg_train_loss_epoch = total_train_loss_epoch / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} - Average Epoch Train Loss: {avg_train_loss_epoch:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "    return train_losses, val_losses, log_steps_recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geyu/projects/my-test/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-large-960h\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2ForCTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m RANDOM_STATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m \u001b[38;5;66;03m# to ensure reproducibility\u001b[39;00m\n\u001b[1;32m      6\u001b[0m TRAIN_SPLIT_RATIO \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3381\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   3379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3380\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 3381\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3383\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   3384\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   3385\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   3386\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   3387\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:519\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m         map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/torch/serialization.py:1516\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:532\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ):\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    530\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m         )\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    534\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/torch/serialization.py:2078\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2077\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 2078\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/projects/my-test/.venv/lib/python3.10/site-packages/torch/serialization.py:2031\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2024\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_offset \u001b[38;5;241m!=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_record_offset(name):\n\u001b[1;32m   2025\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable was set: Incorrect offset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file\u001b[38;5;241m.\u001b[39mget_record_offset(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2029\u001b[0m             )\n\u001b[1;32m   2030\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2031\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2032\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[1;32m   2033\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   2034\u001b[0m     )\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
