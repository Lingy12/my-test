{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from tqdm.auto import tqdm\n",
    "import torchaudio\n",
    "from dataclasses import dataclass, field\n",
    "import evaluate\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'facebook/wav2vec2-base-960h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "origin_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(device)\n",
    "origin_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../data'\n",
    "TEST_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-test.csv')\n",
    "TEST_DATA_BASE = f\"{ROOT_DIR}/cv-valid-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(file, eval_model, processor):\n",
    "    # try:\n",
    "    # Load audio file with soundfile\n",
    "    audio_array, sample_rate = sf.read(file)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if len(audio_array.shape) > 1:\n",
    "        audio_array = audio_array.mean(axis=1)\n",
    "    \n",
    "    # Resample to 16kHz if needed\n",
    "    if sample_rate != 16000:\n",
    "        # Calculate new length for 16kHz\n",
    "        new_length = int(len(audio_array) * 16000 / sample_rate)\n",
    "        audio_array = np.interp(\n",
    "            np.linspace(0, len(audio_array), new_length),\n",
    "            np.arange(len(audio_array)),\n",
    "            audio_array\n",
    "        )\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    # Get duration\n",
    "    \n",
    "    # Normalize audio array\n",
    "    audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "    \n",
    "        # Process audio with Wav2Vec2\n",
    "    input_values = processor(\n",
    "        audio_array, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"longest\",\n",
    "        sampling_rate=sample_rate\n",
    "    ).input_values.to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        logits = eval_model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(eval_model, eval_processor, data_base, data_indices):\n",
    "    references, predictions = [], []\n",
    "    for i in tqdm(range(len(data_indices))):\n",
    "        row = data_indices.iloc[i]\n",
    "        file_path = f\"{data_base}/{row['filename']}\"\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File {file_path} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        transcription = transcribe(file_path, eval_model, eval_processor)\n",
    "        \n",
    "        # print(transcription)\n",
    "        # print(row)\n",
    "        references.append(row['text'].upper())\n",
    "        predictions.append(transcription)\n",
    "        # break\n",
    "\n",
    "    performance = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"Word Error Rate: {performance:.4f}\")    \n",
    "    return performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINTUNED_MODEL = '../models/wav2vec2-large-960h-cv/best_model'\n",
    "cv_model = Wav2Vec2ForCTC.from_pretrained(FINTUNED_MODEL).to(device)\n",
    "cv_processor = Wav2Vec2Processor.from_pretrained(FINTUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1152/3995 [00:29<01:12, 39.47it/s]/tmp/ipykernel_2917517/3600475564.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  audio_array = audio_array / np.max(np.abs(audio_array))\n",
      "100%|██████████| 3995/3995 [01:38<00:00, 40.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3995/3995 [01:37<00:00, 40.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.0826\n"
     ]
    }
   ],
   "source": [
    "wer_origin = evaluate_model(origin_model, origin_processor, TEST_DATA_BASE, TEST_INDICES)\n",
    "wer_finetuned = evaluate_model(cv_model, cv_processor, TEST_DATA_BASE, TEST_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER for original model: 0.1041\n",
      "WER for finetuned model: 0.0826\n"
     ]
    }
   ],
   "source": [
    "print(f\"WER for original model: {wer_origin:.4f}\")\n",
    "print(f\"WER for finetuned model: {wer_finetuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-dev.csv')\n",
    "DEV_DATA_BASE = f\"{ROOT_DIR}/cv-valid-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 1371/4076 [00:33<01:12, 37.21it/s]/tmp/ipykernel_2917517/3600475564.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  audio_array = audio_array / np.max(np.abs(audio_array))\n",
      "100%|██████████| 4076/4076 [01:46<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4076/4076 [02:47<00:00, 24.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.0861\n"
     ]
    }
   ],
   "source": [
    "wer_origin = evaluate_model(origin_model, origin_processor, DEV_DATA_BASE, DEV_INDICES)\n",
    "wer_finetuned = evaluate_model(cv_model, cv_processor, DEV_DATA_BASE, DEV_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
