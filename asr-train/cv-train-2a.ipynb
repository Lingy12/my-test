{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import evaluate\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "\n",
    "1. Load the audio from corresponding path\n",
    "2. resample the audio to 16kHz\n",
    "3. Ensure the audio is mono channel, if not, take the mean of the channels\n",
    "4. using wav2vec processor to process the audio since we only finetuning on English, hence no special tokenizer need to be use. \n",
    "5. remove all punctuation from the transcription and use upper case (match the original training data style)\n",
    "6. Remove the audio that longer than 20 seconds, to prevent OOM\n",
    "\n",
    "Training\n",
    "\n",
    "\n",
    "The learning rate is selected by sub-sample the validation samples, and run for a few hundrud steps. Use the learning rate that has the best convergence and best validation wer. \n",
    "\n",
    "Batch size is selected that largest to fit the GPU VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../data'\n",
    "TRAIN_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-train.csv')\n",
    "TRAIN_DATA_BASE = f\"{ROOT_DIR}/cv-valid-train\"\n",
    "\n",
    "# TRAIN_INDICES = TRAIN_INDICES.sample(frac=0.01, random_state=42)\n",
    "\n",
    "TEST_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-test.csv')\n",
    "TEST_DATA_BASE = f\"{ROOT_DIR}/cv-valid-test\"\n",
    "\n",
    "'''\n",
    "1. Load the audio from corresponding path\n",
    "2. resample the audio to 16kHz\n",
    "3. Ensure the audio is mono channel, if not, take the mean of the channels\n",
    "4. using wav2vec processor to process the audio\n",
    "5. remove all punctuation from the transcription (match the original training data style)\n",
    "6. Remove the audio that longer than 20 seconds, to prevent OOM\n",
    "'''\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, audio_base_path):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.audio_base_path = audio_base_path\n",
    "        self.chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"“%”\\\\\\]'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        audio_filename = row['filename']\n",
    "        text = row['text']\n",
    "\n",
    "        # Construct full audio file path\n",
    "        audio_path = os.path.join(self.audio_base_path, audio_filename)\n",
    "\n",
    "        # Load audio file\n",
    "        try:\n",
    "            speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "            return None # Return None for problematic samples, will be filtered by collator\n",
    "\n",
    "        # Resample if necessary (Wav2Vec2 expects 16kHz)\n",
    "        if sampling_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            speech_array = resampler(speech_array)\n",
    "        \n",
    "        # Ensure mono audio\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = speech_array.mean(dim=0, keepdim=True)\n",
    "\n",
    "        if len(speech_array.squeeze(0)) > 16000 * 20:  # If audio is longer than 20 seconds\n",
    "            print(f\"Audio file {audio_path} is too long, skipping.\")\n",
    "            return None\n",
    "            \n",
    "        # Process audio (normalize, etc.)\n",
    "        # Squeeze to remove the channel dimension (e.g., from (1, samples) to (samples,))\n",
    "        input_values = self.processor(speech_array.squeeze(0).numpy(), sampling_rate=16000).input_values[0]\n",
    "\n",
    "        # Clean and tokenize text\n",
    "        text = re.sub(self.chars_to_ignore_regex, '', text).upper()\n",
    "        \n",
    "        \n",
    "        # print(text)\n",
    "        return {\"input_values\": input_values, \"labels\": text}\n",
    "    \n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Filter out None values from __getitem__\n",
    "        features = [f for f in features if f is not None]\n",
    "        if not features: # If all features were None, return empty batch\n",
    "            return None\n",
    "        # print(self.processor.tokenizer.encode(features[0]['labels']))\n",
    "        # Separate input_values and labels\n",
    "        input_features = [feature[\"input_values\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "   \n",
    "        batch = self.processor(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "  \n",
    "        labels_batch = self.processor(\n",
    "            text = label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # print(labels_batch['input_ids'])\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "'''\n",
    "The learning rate is selected by sub-sample the validation samples, and run for a few hundrud steps. Use the learning rate that has the best convergence and best validation wer. \n",
    "\n",
    "Batch size is selected that largest to fit the GPU VRAM\n",
    "'''\n",
    "\n",
    "def train(run_final_validation: bool = True):\n",
    "    MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME, sampling_rate=16000, ctc_loss_reduction=\"mean\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME)\n",
    "    RANDOM_STATE = 42 # to ensure reproducibility\n",
    "    TRAIN_SPLIT_RATIO = 0.7\n",
    "    LEARNING_RATE = 1e-5\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    VAL_BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 3\n",
    "    # EVAL_STEP: Controls how often validation is run and metrics are evaluated\n",
    "    EVAL_STEP = 1000\n",
    "    # LOG_STEP: Controls how often training loss is logged to console\n",
    "    LOG_STEP = 50 # Log training loss every 50 steps\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    MAX_STEP_CHECKPOINTS = 3 # Maximum number of step checkpoints to keep\n",
    "    EARLY_STOP_PATIENCE = 3 # Number of validation checks to wait before early stopping\n",
    "\n",
    "    # Adjust EVAL_STEP to account for gradient accumulation\n",
    "    EVAL_STEP = EVAL_STEP * GRADIENT_ACCUMULATION_STEPS\n",
    "    print(f\"Using gradient accumulation with {GRADIENT_ACCUMULATION_STEPS} steps, effective EVAL_STEP for validation is now {EVAL_STEP}\")\n",
    "\n",
    "    # Lists to store metrics\n",
    "    # train_losses_logged will store training loss averaged over LOG_STEP intervals\n",
    "    train_losses_logged = []\n",
    "    # train_losses_at_eval will store training loss averaged over EVAL_STEP intervals (for comparison with validation)\n",
    "    train_losses_at_eval = []\n",
    "    val_losses = []\n",
    "    val_wer_scores = []\n",
    "    # log_steps_recorded will correspond to the steps where validation (EVAL_STEP) was performed\n",
    "    log_steps_recorded = []\n",
    "    val_results = []\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        TRAIN_INDICES, test_size=1-TRAIN_SPLIT_RATIO, random_state=RANDOM_STATE\n",
    "    )\n",
    "    # val_indices = val_indices.sample(n=1000, random_state=RANDOM_STATE)  # Limit validation set to 1000 samples\n",
    "    print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "\n",
    "    # Freeze feature extractor layers\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_dataset = AudioDataset(train_indices, processor, TRAIN_DATA_BASE)\n",
    "    val_dataset = AudioDataset(val_indices, processor, TRAIN_DATA_BASE)\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=32, # Set to 0 for simpler debugging, adjust for production\n",
    "        pin_memory=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=32, # Set to 0 for simpler debugging, adjust for production\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scaler = torch.cuda.amp.GradScaler() # use mixed presition for speed up\n",
    "\n",
    "    # Initialize WER metric\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    # Accumulators for training loss, separated for LOG_STEP and EVAL_STEP\n",
    "    accumulated_loss_for_log_step = 0 # For logging train loss more frequently\n",
    "    batches_in_log_interval = 0\n",
    "\n",
    "    accumulated_loss_for_eval_step = 0 # For calculating train loss at validation points\n",
    "\n",
    "    # Initialize variables for best model saving\n",
    "    best_wer = float('inf')\n",
    "    model_save_base_path = \"../models/checkpoints\"\n",
    "    os.makedirs(model_save_base_path, exist_ok=True) # Ensure base directory exists\n",
    "    best_model_save_path = os.path.join(model_save_base_path, \"best_model\")\n",
    "\n",
    "    # List to keep track of saved step checkpoint paths for rotation\n",
    "    saved_step_checkpoints = []\n",
    "\n",
    "    # Early stopping variables\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Track the last step reached for potential final validation\n",
    "    last_step_reached = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        total_train_loss_epoch = 0 # For overall epoch average train loss\n",
    "        optimizer.zero_grad() # Ensure gradients are zeroed at the start of each epoch\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", file=sys.stdout)):\n",
    "            current_step = (epoch * len(train_dataloader)) + (i + 1)\n",
    "            last_step_reached = current_step # Update last step reached\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_values=batch[\"input_values\"], labels=batch[\"labels\"])\n",
    "            loss = outputs.loss\n",
    "            # Scale the loss by the number of accumulation steps\n",
    "            scaled_loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            scaler.scale(scaled_loss).backward()\n",
    "\n",
    "            # Accumulate loss for epoch average\n",
    "            total_train_loss_epoch += loss.item()\n",
    "\n",
    "            # Accumulate loss for logging at LOG_STEP intervals\n",
    "            accumulated_loss_for_log_step += loss.item()\n",
    "            batches_in_log_interval += 1\n",
    "\n",
    "            # Accumulate loss for logging at EVAL_STEP intervals\n",
    "            accumulated_loss_for_eval_step += loss.item()\n",
    "\n",
    "            # Perform optimizer step and zero gradients only after accumulating enough gradients\n",
    "            if current_step % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() # Zero gradients after the optimizer step\n",
    "\n",
    "            # --- Logging Training Loss (Every LOG_STEP) ---\n",
    "            if current_step % LOG_STEP == 0:\n",
    "                # Calculate average accumulated loss over the LOG_STEP interval\n",
    "                avg_train_loss_current_log_interval = accumulated_loss_for_log_step / batches_in_log_interval\n",
    "                train_losses_logged.append(avg_train_loss_current_log_interval)\n",
    "                print(f\"Step {current_step} - Train Loss (last {batches_in_log_interval} batches): {avg_train_loss_current_log_interval:.4f}\")\n",
    "                # Reset accumulator for the next LOG_STEP interval\n",
    "                accumulated_loss_for_log_step = 0\n",
    "                batches_in_log_interval = 0\n",
    "\n",
    "            # --- Validation and WER Calculation Phase (Every EVAL_STEP) ---\n",
    "            if current_step % EVAL_STEP == 0:\n",
    "                # Calculate average training loss over the EVAL_STEP interval\n",
    "                # Divide by the actual number of batches in the EVAL_STEP interval\n",
    "                current_train_loss_for_eval = accumulated_loss_for_eval_step / (EVAL_STEP / GRADIENT_ACCUMULATION_STEPS)\n",
    "                train_losses_at_eval.append(current_train_loss_for_eval)\n",
    "                accumulated_loss_for_eval_step = 0 # Reset for next EVAL_STEP\n",
    "\n",
    "                model.eval() # Set model to evaluation mode\n",
    "                total_val_loss = 0\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Ensure tqdm prints to sys.stdout for consistency\n",
    "                    for val_batch in tqdm(val_dataloader, desc=f\"Step {current_step} Validation\", file=sys.stdout):\n",
    "                        val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
    "                        outputs = model(input_values=val_batch[\"input_values\"], labels=val_batch[\"labels\"])\n",
    "                        val_loss = outputs.loss\n",
    "                        total_val_loss += val_loss.item()\n",
    "\n",
    "                        # Calculate WER\n",
    "                        logits = outputs.logits\n",
    "                        pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                        # Replace -100 in labels with processor.tokenizer.pad_token_id\n",
    "                        val_batch[\"labels\"][val_batch[\"labels\"] == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "                        # Decode predictions and labels\n",
    "                        predictions = processor.batch_decode(pred_ids)\n",
    "                        references = processor.batch_decode(val_batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "                        all_preds.extend(predictions)\n",
    "                        all_labels.extend(references)\n",
    "\n",
    "                avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "\n",
    "                # Compute WER for the entire validation set at this eval step\n",
    "                wer_score = wer_metric.compute(predictions=all_preds, references=all_labels)\n",
    "                val_wer_scores.append(wer_score)\n",
    "                val_results.append({\n",
    "                    \"step\": current_step,\n",
    "                    \"validation_loss\": avg_val_loss,\n",
    "                    \"wer\": wer_score,\n",
    "                    \"predictions\": all_preds,\n",
    "                    \"references\": all_labels\n",
    "                })\n",
    "                log_steps_recorded.append(current_step) # These steps correspond to validation runs\n",
    "\n",
    "                print(f\"Step {current_step} - Train Loss (avg over last {EVAL_STEP} batches): {current_train_loss_for_eval:.4f} - Validation Loss: {avg_val_loss:.4f} - Validation WER: {wer_score:.4f}\")\n",
    "\n",
    "                # --- Early Stopping Logic ---\n",
    "                if wer_score < best_wer:\n",
    "                    best_wer = wer_score\n",
    "                    epochs_no_improve = 0\n",
    "                    # Create directory for the best model if it doesn't exist\n",
    "                    os.makedirs(best_model_save_path, exist_ok=True)\n",
    "                    model.save_pretrained(best_model_save_path)\n",
    "                    processor.save_pretrained(best_model_save_path)\n",
    "                    print(f\"New best model saved at step {current_step} with WER: {best_wer:.4f} to {best_model_save_path}\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"Validation WER did not improve for {epochs_no_improve} times.\")\n",
    "                    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "                        print(f\"Early stopping: Validation WER has not improved for {EARLY_STOP_PATIENCE} consecutive validation checks. Stopping training.\")\n",
    "                        early_stop = True\n",
    "                        break # Break from the inner loop (training steps)\n",
    "\n",
    "                # --- Save model at every evaluation step (with rotation) ---\n",
    "                current_step_model_path = os.path.join(model_save_base_path, f\"model_step_{current_step}\")\n",
    "                os.makedirs(current_step_model_path, exist_ok=True)\n",
    "                model.save_pretrained(current_step_model_path)\n",
    "                processor.save_pretrained(current_step_model_path)\n",
    "                print(f\"Model saved for step {current_step} to {current_step_model_path}\")\n",
    "\n",
    "                # Add the new checkpoint path to the list\n",
    "                saved_step_checkpoints.append(current_step_model_path)\n",
    "\n",
    "                # If more than MAX_STEP_CHECKPOINTS are saved, remove the oldest one\n",
    "                if len(saved_step_checkpoints) > MAX_STEP_CHECKPOINTS:\n",
    "                    oldest_checkpoint_path = saved_step_checkpoints.pop(0) # Remove the oldest path\n",
    "                    if os.path.exists(oldest_checkpoint_path):\n",
    "                        shutil.rmtree(oldest_checkpoint_path) # Delete the directory\n",
    "                        print(f\"Removed oldest checkpoint: {oldest_checkpoint_path}\")\n",
    "\n",
    "                model.train() # Set model back to training mode\n",
    "\n",
    "        # Handle any remaining gradients if the total number of batches is not a multiple of GRADIENT_ACCUMULATION_STEPS\n",
    "        # and if training is not stopping immediately\n",
    "        if not early_stop and (current_step % GRADIENT_ACCUMULATION_STEPS != 0):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if early_stop: # Check again after the inner loop in case it broke due to early stopping\n",
    "            break\n",
    "\n",
    "        avg_train_loss_epoch = total_train_loss_epoch / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} - Average Epoch Train Loss: {avg_train_loss_epoch:.4f}\")\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "    # --- Final Validation (Optional) ---\n",
    "    if run_final_validation:\n",
    "        print(f\"\\nRunning final validation at step {last_step_reached} (controlled by run_final_validation={run_final_validation})...\")\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in tqdm(val_dataloader, desc=f\"Final Validation\", file=sys.stdout):\n",
    "                val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
    "                outputs = model(input_values=val_batch[\"input_values\"], labels=val_batch[\"labels\"])\n",
    "                val_loss = outputs.loss\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                logits = outputs.logits\n",
    "                pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_batch[\"labels\"][val_batch[\"labels\"] == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "                predictions = processor.batch_decode(pred_ids)\n",
    "                references = processor.batch_decode(val_batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "                all_preds.extend(predictions)\n",
    "                all_labels.extend(references)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        wer_score = wer_metric.compute(predictions=all_preds, references=all_labels)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_wer_scores.append(wer_score)\n",
    "        val_results.append({\n",
    "            \"step\": last_step_reached,\n",
    "            \"validation_loss\": avg_val_loss,\n",
    "            \"wer\": wer_score,\n",
    "            \"predictions\": all_preds,\n",
    "            \"references\": all_labels\n",
    "        })\n",
    "        log_steps_recorded.append(last_step_reached)\n",
    "\n",
    "        print(f\"Final Validation Results (Step {last_step_reached}): Loss: {avg_val_loss:.4f}, WER: {wer_score:.4f}\")\n",
    "        \n",
    "        if wer_score < best_wer:\n",
    "            best_wer = wer_score\n",
    "            # Create directory for the best model if it doesn't exist\n",
    "            os.makedirs(best_model_save_path, exist_ok=True)\n",
    "            model.save_pretrained(best_model_save_path)\n",
    "            processor.save_pretrained(best_model_save_path)\n",
    "            print(f\"Final best model saved at step {last_step_reached} with WER: {best_wer:.4f} to {best_model_save_path}\")\n",
    "\n",
    "    # The model saved as 'best_model' is the one with the lowest WER throughout training.\n",
    "    # We explicitly save the final state of the model as 'final_model' as well.\n",
    "    final_model_save_path = os.path.join(model_save_base_path, \"final_model\")\n",
    "    os.makedirs(final_model_save_path, exist_ok=True)\n",
    "    model.save_pretrained(final_model_save_path)\n",
    "    processor.save_pretrained(final_model_save_path)\n",
    "    print(f\"Final model saved to {final_model_save_path}\")\n",
    "\n",
    "    # Return the losses appropriate for logging and validation\n",
    "    return train_losses_logged, val_losses, log_steps_recorded, val_wer_scores, val_results\n",
    "\n",
    "\n",
    "\n",
    "train_losses, val_losses, log_steps_recorded, val_wer_scores, val_results = train()\n",
    "\n",
    "with open('./results.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump({\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"log_steps_recorded\": log_steps_recorded,\n",
    "        \"val_wer_scores\": val_wer_scores,\n",
    "        \"val_results\": val_results\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv ../models/checkpoints/best-model ../models/wav2vec2-large-960h-cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "    # print(\"Training Losses:\", results['train_losses'])\n",
    "    # print(\"Validation Losses:\", results['val_losses'])\n",
    "    # print(\"Log Steps Recorded:\", results['log_steps_recorded'])\n",
    "    # print(\"Validation WER Scores:\", results['val_wer_scores'])\n",
    "    # print(\"Validation Results:\", results['val_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "MODEL = 'facebook/wav2vec2-base-960h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "origin_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(device)\n",
    "origin_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../data'\n",
    "TEST_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-test.csv')\n",
    "TEST_DATA_BASE = f\"{ROOT_DIR}/cv-valid-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(file, eval_model, processor):\n",
    "    # try:\n",
    "    # Load audio file with soundfile\n",
    "    audio_array, sample_rate = sf.read(file)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if len(audio_array.shape) > 1:\n",
    "        audio_array = audio_array.mean(axis=1)\n",
    "    \n",
    "    # Resample to 16kHz if needed\n",
    "    if sample_rate != 16000:\n",
    "        # Calculate new length for 16kHz\n",
    "        new_length = int(len(audio_array) * 16000 / sample_rate)\n",
    "        audio_array = np.interp(\n",
    "            np.linspace(0, len(audio_array), new_length),\n",
    "            np.arange(len(audio_array)),\n",
    "            audio_array\n",
    "        )\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    # Get duration\n",
    "    \n",
    "    # Normalize audio array\n",
    "    audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "    \n",
    "        # Process audio with Wav2Vec2\n",
    "    input_values = processor(\n",
    "        audio_array, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"longest\",\n",
    "        sampling_rate=sample_rate\n",
    "    ).input_values.to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        logits = eval_model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(eval_model, eval_processor, data_base, data_indices):\n",
    "    references, predictions = [], []\n",
    "    for i in tqdm(range(len(data_indices))):\n",
    "        row = data_indices.iloc[i]\n",
    "        file_path = f\"{data_base}/{row['filename']}\"\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File {file_path} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        transcription = transcribe(file_path, eval_model, eval_processor)\n",
    "        \n",
    "        # print(transcription)\n",
    "        # print(row)\n",
    "        references.append(row['text'].upper())\n",
    "        predictions.append(transcription)\n",
    "        # break\n",
    "\n",
    "    performance = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"Word Error Rate: {performance:.4f}\")    \n",
    "    return performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geyu/projects/my-test/.venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "FINTUNED_MODEL = '../models/wav2vec2-large-960h-cv'\n",
    "cv_model = Wav2Vec2ForCTC.from_pretrained(FINTUNED_MODEL).to(device)\n",
    "cv_processor = Wav2Vec2Processor.from_pretrained(FINTUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1155/3995 [00:27<01:08, 41.59it/s]/tmp/ipykernel_2966536/3600475564.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  audio_array = audio_array / np.max(np.abs(audio_array))\n",
      "100%|██████████| 3995/3995 [01:33<00:00, 42.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3995/3995 [01:32<00:00, 43.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.0608\n"
     ]
    }
   ],
   "source": [
    "wer_origin = evaluate_model(origin_model, origin_processor, TEST_DATA_BASE, TEST_INDICES)\n",
    "wer_finetuned = evaluate_model(cv_model, cv_processor, TEST_DATA_BASE, TEST_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER for original model: 0.1041\n",
      "WER for finetuned model: 0.0608\n"
     ]
    }
   ],
   "source": [
    "print(f\"WER for original model: {wer_origin:.4f}\")\n",
    "print(f\"WER for finetuned model: {wer_finetuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_INDICES = pd.read_csv(f'{ROOT_DIR}/cv-valid-dev.csv')\n",
    "DEV_DATA_BASE = f\"{ROOT_DIR}/cv-valid-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 1372/4076 [00:31<01:05, 41.00it/s]/tmp/ipykernel_2966536/3600475564.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  audio_array = audio_array / np.max(np.abs(audio_array))\n",
      "100%|██████████| 4076/4076 [01:33<00:00, 43.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4076/4076 [01:35<00:00, 42.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate: 0.0641\n"
     ]
    }
   ],
   "source": [
    "wer_origin = evaluate_model(origin_model, origin_processor, DEV_DATA_BASE, DEV_INDICES)\n",
    "wer_finetuned = evaluate_model(cv_model, cv_processor, DEV_DATA_BASE, DEV_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
